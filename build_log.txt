#0 building with "desktop-linux" instance using docker driver

#1 [internal] load build definition from Dockerfile
#1 transferring dockerfile: 4.31kB done
#1 DONE 0.0s

#2 [auth] library/python:pull token for registry-1.docker.io
#2 DONE 0.0s

#3 [internal] load metadata for docker.io/library/python:3.11-slim
#3 DONE 0.8s

#4 [internal] load .dockerignore
#4 transferring context: 1.78kB done
#4 DONE 0.0s

#5 [1/9] FROM docker.io/library/python:3.11-slim@sha256:dbf1de478a55d6763afaa39c2f3d7b54b25230614980276de5cacdde79529d0c
#5 resolve docker.io/library/python:3.11-slim@sha256:dbf1de478a55d6763afaa39c2f3d7b54b25230614980276de5cacdde79529d0c 0.1s done
#5 DONE 0.1s

#6 [2/9] WORKDIR /app
#6 CACHED

#7 [internal] load build context
#7 transferring context: 199.24kB 0.1s done
#7 DONE 0.1s

#8 [3/9] COPY requirements.txt .
#8 DONE 0.0s

#9 [4/9] RUN pip install --no-cache-dir --upgrade pip -r requirements.txt
#9 1.418 Requirement already satisfied: pip in /usr/local/lib/python3.11/site-packages (24.0)
#9 1.555 Collecting pip
#9 1.656   Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)
#9 1.686 Collecting cloudpickle==3.1.1 (from -r requirements.txt (line 1))
#9 1.708   Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)
#9 1.753 Collecting Flask==3.1.1 (from -r requirements.txt (line 2))
#9 1.771   Downloading flask-3.1.1-py3-none-any.whl.metadata (3.0 kB)
#9 1.803 Collecting gunicorn==23.0.0 (from -r requirements.txt (line 3))
#9 1.821   Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)
#9 1.879 Collecting joblib==1.5.0 (from -r requirements.txt (line 4))
#9 1.896   Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)
#9 2.006 Collecting lightgbm==4.6.0 (from -r requirements.txt (line 5))
#9 2.024   Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)
#9 2.182 Collecting matplotlib==3.10.3 (from -r requirements.txt (line 6))
#9 2.200   Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)
#9 2.256 Collecting mlflow==2.22.0 (from -r requirements.txt (line 7))
#9 2.275   Downloading mlflow-2.22.0-py3-none-any.whl.metadata (30 kB)
#9 2.489 Collecting numpy==2.2.6 (from -r requirements.txt (line 8))
#9 2.507   Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)
#9 2.516      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.0/62.0 kB 7.5 MB/s eta 0:00:00
#9 2.646 Collecting pandas==2.2.3 (from -r requirements.txt (line 9))
#9 2.663   Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)
#9 2.669      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.9/89.9 kB 22.7 MB/s eta 0:00:00
#9 2.752 Collecting pytest<9.0.0,>=8.0.0 (from -r requirements.txt (line 10))
#9 2.770   Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)
#9 2.948 Collecting protobuf==6.31.0 (from -r requirements.txt (line 11))
#9 2.968   Downloading protobuf-6.31.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)
#9 3.016 Collecting PyYAML==6.0.2 (from -r requirements.txt (line 12))
#9 3.034   Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)
#9 3.077 Collecting requests==2.32.3 (from -r requirements.txt (line 13))
#9 3.094   Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)
#9 3.216 Collecting scikit-learn==1.6.1 (from -r requirements.txt (line 14))
#9 3.233   Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)
#9 3.356 Collecting scipy==1.15.3 (from -r requirements.txt (line 15))
#9 3.374   Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)
#9 3.377      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.0/62.0 kB 41.7 MB/s eta 0:00:00
#9 3.677 Collecting SQLAlchemy==2.0.41 (from -r requirements.txt (line 16))
#9 3.695   Downloading sqlalchemy-2.0.41-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)
#9 3.721 Collecting threadpoolctl==3.6.0 (from -r requirements.txt (line 17))
#9 3.738   Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)
#9 3.769 Collecting blinker>=1.9.0 (from Flask==3.1.1->-r requirements.txt (line 2))
#9 3.787   Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)
#9 3.817 Collecting click>=8.1.3 (from Flask==3.1.1->-r requirements.txt (line 2))
#9 3.834   Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)
#9 3.858 Collecting itsdangerous>=2.2.0 (from Flask==3.1.1->-r requirements.txt (line 2))
#9 3.875   Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)
#9 3.943 Collecting jinja2>=3.1.2 (from Flask==3.1.1->-r requirements.txt (line 2))
#9 3.960   Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
#9 4.023 Collecting markupsafe>=2.1.1 (from Flask==3.1.1->-r requirements.txt (line 2))
#9 4.040   Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)
#9 4.084 Collecting werkzeug>=3.1.0 (from Flask==3.1.1->-r requirements.txt (line 2))
#9 4.101   Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)
#9 4.140 Collecting packaging (from gunicorn==23.0.0->-r requirements.txt (line 3))
#9 4.157   Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)
#9 4.253 Collecting contourpy>=1.0.1 (from matplotlib==3.10.3->-r requirements.txt (line 6))
#9 4.272   Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)
#9 4.295 Collecting cycler>=0.10 (from matplotlib==3.10.3->-r requirements.txt (line 6))
#9 4.312   Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)
#9 4.468 Collecting fonttools>=4.22.0 (from matplotlib==3.10.3->-r requirements.txt (line 6))
#9 4.486   Downloading fonttools-4.58.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (106 kB)
#9 4.493      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.1/106.1 kB 21.1 MB/s eta 0:00:00
#9 4.563 Collecting kiwisolver>=1.3.1 (from matplotlib==3.10.3->-r requirements.txt (line 6))
#9 4.581   Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)
#9 4.825 Collecting pillow>=8 (from matplotlib==3.10.3->-r requirements.txt (line 6))
#9 4.842   Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)
#9 4.888 Collecting pyparsing>=2.3.1 (from matplotlib==3.10.3->-r requirements.txt (line 6))
#9 4.905   Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)
#9 4.932 Collecting python-dateutil>=2.7 (from matplotlib==3.10.3->-r requirements.txt (line 6))
#9 4.950   Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)
#9 5.024 Collecting mlflow-skinny==2.22.0 (from mlflow==2.22.0->-r requirements.txt (line 7))
#9 5.043   Downloading mlflow_skinny-2.22.0-py3-none-any.whl.metadata (31 kB)
#9 5.108 Collecting alembic!=1.10.0,<2 (from mlflow==2.22.0->-r requirements.txt (line 7))
#9 5.126   Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)
#9 5.164 Collecting docker<8,>=4.0.0 (from mlflow==2.22.0->-r requirements.txt (line 7))
#9 5.181   Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)
#9 5.211 Collecting graphene<4 (from mlflow==2.22.0->-r requirements.txt (line 7))
#9 5.229   Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)
#9 5.269 Collecting markdown<4,>=3.3 (from mlflow==2.22.0->-r requirements.txt (line 7))
#9 5.286   Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)
#9 5.394 Collecting pyarrow<20,>=4.0.0 (from mlflow==2.22.0->-r requirements.txt (line 7))
#9 5.413   Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)
#9 5.546 Collecting pytz>=2020.1 (from pandas==2.2.3->-r requirements.txt (line 9))
#9 5.563   Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)
#9 5.594 Collecting tzdata>=2022.7 (from pandas==2.2.3->-r requirements.txt (line 9))
#9 5.612   Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)
#9 5.710 Collecting charset-normalizer<4,>=2 (from requests==2.32.3->-r requirements.txt (line 13))
#9 5.727   Downloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)
#9 5.755 Collecting idna<4,>=2.5 (from requests==2.32.3->-r requirements.txt (line 13))
#9 5.773   Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)
#9 5.824 Collecting urllib3<3,>=1.21.1 (from requests==2.32.3->-r requirements.txt (line 13))
#9 5.841   Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)
#9 5.874 Collecting certifi>=2017.4.17 (from requests==2.32.3->-r requirements.txt (line 13))
#9 5.891   Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)
#9 6.199 Collecting greenlet>=1 (from SQLAlchemy==2.0.41->-r requirements.txt (line 16))
#9 6.219   Downloading greenlet-3.2.2-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)
#9 6.251 Collecting typing-extensions>=4.6.0 (from SQLAlchemy==2.0.41->-r requirements.txt (line 16))
#9 6.268   Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)
#9 6.333 Collecting cachetools<6,>=5.0.0 (from mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 6.350   Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)
#9 6.400 Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 6.418   Downloading databricks_sdk-0.55.0-py3-none-any.whl.metadata (39 kB)
#9 6.498 Collecting fastapi<1 (from mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 6.515   Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)
#9 6.562 Collecting gitpython<4,>=3.1.9 (from mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 6.579   Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)
#9 6.651 Collecting importlib_metadata!=4.7.0,<9,>=3.7.0 (from mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 6.669   Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)
#9 6.708 Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 6.725   Downloading opentelemetry_api-1.33.1-py3-none-any.whl.metadata (1.6 kB)
#9 6.769 Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 6.788   Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl.metadata (1.6 kB)
#9 6.797 Collecting packaging (from gunicorn==23.0.0->-r requirements.txt (line 3))
#9 6.815   Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)
#9 7.035 Collecting pydantic<3,>=1.10.8 (from mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 7.052   Downloading pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)
#9 7.057      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.2/67.2 kB 20.8 MB/s eta 0:00:00
#9 7.095 Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 7.114   Downloading sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)
#9 7.166 Collecting uvicorn<1 (from mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 7.185   Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)
#9 7.223 Collecting iniconfig (from pytest<9.0.0,>=8.0.0->-r requirements.txt (line 10))
#9 7.240   Downloading iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)
#9 7.269 Collecting pluggy<2,>=1.5 (from pytest<9.0.0,>=8.0.0->-r requirements.txt (line 10))
#9 7.286   Downloading pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)
#9 7.325 Collecting Mako (from alembic!=1.10.0,<2->mlflow==2.22.0->-r requirements.txt (line 7))
#9 7.344   Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)
#9 7.483 Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow==2.22.0->-r requirements.txt (line 7))
#9 7.501   Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)
#9 7.529 Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow==2.22.0->-r requirements.txt (line 7))
#9 7.548   Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)
#9 7.673 Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib==3.10.3->-r requirements.txt (line 6))
#9 7.691   Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)
#9 7.850 Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 7.868   Downloading google_auth-2.40.2-py2.py3-none-any.whl.metadata (6.2 kB)
#9 7.941 Collecting starlette<0.47.0,>=0.40.0 (from fastapi<1->mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 7.958   Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)
#9 8.040 Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 8.058   Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)
#9 8.134 Collecting zipp>=3.20 (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 8.151   Downloading zipp-3.22.0-py3-none-any.whl.metadata (3.6 kB)
#9 8.194 Collecting deprecated>=1.2.6 (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 8.217   Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)
#9 8.236 Collecting importlib_metadata!=4.7.0,<9,>=3.7.0 (from mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 8.253   Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)
#9 8.320 Collecting opentelemetry-semantic-conventions==0.54b1 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 8.338   Downloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl.metadata (2.5 kB)
#9 8.388 Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 8.407   Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)
#9 9.075 Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 9.095   Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)
#9 9.123 Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 9.140   Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)
#9 9.213 Collecting h11>=0.8 (from uvicorn<1->mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 9.230   Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)
#9 9.371 Collecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 9.388   Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)
#9 9.426 Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 9.444   Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)
#9 9.510 Collecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 9.527   Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)
#9 9.555 Collecting rsa<5,>=3.1.4 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 9.578   Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)
#9 9.640 Collecting anyio<5,>=3.6.2 (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 9.657   Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)
#9 9.735 Collecting sniffio>=1.1 (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 9.752   Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)
#9 9.808 Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow==2.22.0->-r requirements.txt (line 7))
#9 9.825   Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)
#9 9.912 Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)
#9 9.929 Downloading flask-3.1.1-py3-none-any.whl (103 kB)
#9 9.935    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.3/103.3 kB 23.4 MB/s eta 0:00:00
#9 9.953 Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)
#9 9.958    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.0/85.0 kB 24.3 MB/s eta 0:00:00
#9 9.974 Downloading joblib-1.5.0-py3-none-any.whl (307 kB)
#9 9.993    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 307.7/307.7 kB 18.4 MB/s eta 0:00:00
#9 10.02 Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)
#9 10.14    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 30.7 MB/s eta 0:00:00
#9 10.15 Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)
#9 10.42    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.6/8.6 MB 32.2 MB/s eta 0:00:00
#9 10.44 Downloading mlflow-2.22.0-py3-none-any.whl (29.0 MB)
#9 11.40    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 29.0/29.0 MB 31.1 MB/s eta 0:00:00
#9 11.42 Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)
#9 11.90    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.8/16.8 MB 35.4 MB/s eta 0:00:00
#9 11.92 Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)
#9 12.31    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 33.8 MB/s eta 0:00:00
#9 12.32 Downloading protobuf-6.31.0-cp39-abi3-manylinux2014_x86_64.whl (320 kB)
#9 12.34    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 320.9/320.9 kB 26.1 MB/s eta 0:00:00
#9 12.36 Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)
#9 12.38    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 763.0/763.0 kB 34.2 MB/s eta 0:00:00
#9 12.40 Downloading requests-2.32.3-py3-none-any.whl (64 kB)
#9 12.40    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.9/64.9 kB 43.1 MB/s eta 0:00:00
#9 12.42 Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)
#9 12.90    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.5/13.5 MB 27.7 MB/s eta 0:00:00
#9 12.92 Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)
#9 14.04    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 37.7/37.7 MB 33.3 MB/s eta 0:00:00
#9 14.06 Downloading sqlalchemy-2.0.41-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)
#9 14.16    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 33.0 MB/s eta 0:00:00
#9 14.18 Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)
#9 14.19 Downloading mlflow_skinny-2.22.0-py3-none-any.whl (6.3 MB)
#9 14.39    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.3/6.3 MB 32.3 MB/s eta 0:00:00
#9 14.41 Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)
#9 14.46    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 32.8 MB/s eta 0:00:00
#9 14.48 Downloading pytest-8.3.5-py3-none-any.whl (343 kB)
#9 14.50    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 343.6/343.6 kB 28.1 MB/s eta 0:00:00
#9 14.51 Downloading alembic-1.16.1-py3-none-any.whl (242 kB)
#9 14.52    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 242.5/242.5 kB 27.6 MB/s eta 0:00:00
#9 14.54 Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)
#9 14.56 Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)
#9 14.56    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 159.6/159.6 kB 33.2 MB/s eta 0:00:00
#9 14.58 Downloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)
#9 14.59    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.3/147.3 kB 29.0 MB/s eta 0:00:00
#9 14.60 Downloading click-8.2.1-py3-none-any.whl (102 kB)
#9 14.61    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.2/102.2 kB 39.3 MB/s eta 0:00:00
#9 14.63 Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)
#9 14.64    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 326.2/326.2 kB 28.8 MB/s eta 0:00:00
#9 14.66 Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)
#9 14.69 Downloading docker-7.1.0-py3-none-any.whl (147 kB)
#9 14.72    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.8/147.8 kB 6.3 MB/s eta 0:00:00
#9 14.75 Downloading fonttools-4.58.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)
#9 14.92    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 28.4 MB/s eta 0:00:00
#9 14.94 Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)
#9 14.94    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.9/114.9 kB 24.3 MB/s eta 0:00:00
#9 14.97 Downloading greenlet-3.2.2-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (583 kB)
#9 14.99    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 583.9/583.9 kB 30.0 MB/s eta 0:00:00
#9 15.00 Downloading idna-3.10-py3-none-any.whl (70 kB)
#9 15.01    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70.4/70.4 kB 52.7 MB/s eta 0:00:00
#9 15.02 Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)
#9 15.04 Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)
#9 15.05    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.9/134.9 kB 18.0 MB/s eta 0:00:00
#9 15.07 Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)
#9 15.14    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 22.0 MB/s eta 0:00:00
#9 15.15 Downloading markdown-3.8-py3-none-any.whl (106 kB)
#9 15.16    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.2/106.2 kB 27.1 MB/s eta 0:00:00
#9 15.17 Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)
#9 15.19 Downloading packaging-24.2-py3-none-any.whl (65 kB)
#9 15.20    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 19.3 MB/s eta 0:00:00
#9 15.22 Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)
#9 15.36    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.6/4.6 MB 31.5 MB/s eta 0:00:00
#9 15.38 Downloading pluggy-1.6.0-py3-none-any.whl (20 kB)
#9 15.40 Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)
#9 16.63    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.1/42.1 MB 34.8 MB/s eta 0:00:00
#9 16.64 Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)
#9 16.65    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 111.1/111.1 kB 33.5 MB/s eta 0:00:00
#9 16.67 Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)
#9 16.67    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.9/229.9 kB 27.8 MB/s eta 0:00:00
#9 16.69 Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)
#9 16.71    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 509.2/509.2 kB 29.7 MB/s eta 0:00:00
#9 16.73 Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)
#9 16.73    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.8/45.8 kB 120.1 MB/s eta 0:00:00
#9 16.75 Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)
#9 16.76    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 347.8/347.8 kB 28.5 MB/s eta 0:00:00
#9 16.78 Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)
#9 16.78    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.7/128.7 kB 25.1 MB/s eta 0:00:00
#9 16.80 Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)
#9 16.81    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 26.9 MB/s eta 0:00:00
#9 16.83 Downloading iniconfig-2.1.0-py3-none-any.whl (6.0 kB)
#9 16.84 Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)
#9 16.86 Downloading databricks_sdk-0.55.0-py3-none-any.whl (722 kB)
#9 16.89    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 722.9/722.9 kB 27.8 MB/s eta 0:00:00
#9 16.91 Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)
#9 16.91    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.2/95.2 kB 37.9 MB/s eta 0:00:00
#9 16.93 Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)
#9 16.94    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.6/207.6 kB 28.9 MB/s eta 0:00:00
#9 16.95 Downloading graphql_core-3.2.6-py3-none-any.whl (203 kB)
#9 16.96    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 203.4/203.4 kB 27.3 MB/s eta 0:00:00
#9 16.98 Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)
#9 17.00 Downloading opentelemetry_api-1.33.1-py3-none-any.whl (65 kB)
#9 17.00    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.8/65.8 kB 22.1 MB/s eta 0:00:00
#9 17.02 Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)
#9 17.04 Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl (118 kB)
#9 17.04    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.0/119.0 kB 27.5 MB/s eta 0:00:00
#9 17.06 Downloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl (194 kB)
#9 17.07    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.9/194.9 kB 27.6 MB/s eta 0:00:00
#9 17.09 Downloading pydantic-2.11.5-py3-none-any.whl (444 kB)
#9 17.11    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 444.2/444.2 kB 26.0 MB/s eta 0:00:00
#9 17.12 Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)
#9 17.19    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 31.8 MB/s eta 0:00:00
#9 17.20 Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)
#9 17.22 Downloading sqlparse-0.5.3-py3-none-any.whl (44 kB)
#9 17.22    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.4/44.4 kB 179.4 MB/s eta 0:00:00
#9 17.24 Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)
#9 17.24    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.5/62.5 kB 32.9 MB/s eta 0:00:00
#9 17.26 Downloading mako-1.3.10-py3-none-any.whl (78 kB)
#9 17.27    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.5/78.5 kB 33.4 MB/s eta 0:00:00
#9 17.28 Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)
#9 17.30 Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)
#9 17.32 Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)
#9 17.32    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 26.8 MB/s eta 0:00:00
#9 17.34 Downloading google_auth-2.40.2-py2.py3-none-any.whl (216 kB)
#9 17.35    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 216.1/216.1 kB 24.7 MB/s eta 0:00:00
#9 17.36 Downloading h11-0.16.0-py3-none-any.whl (37 kB)
#9 17.38 Downloading starlette-0.46.2-py3-none-any.whl (72 kB)
#9 17.38    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.0/72.0 kB 29.6 MB/s eta 0:00:00
#9 17.40 Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)
#9 17.42 Downloading zipp-3.22.0-py3-none-any.whl (9.8 kB)
#9 17.44 Downloading anyio-4.9.0-py3-none-any.whl (100 kB)
#9 17.44    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.9/100.9 kB 24.1 MB/s eta 0:00:00
#9 17.46 Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)
#9 17.48    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 11.1 MB/s eta 0:00:00
#9 17.50 Downloading rsa-4.9.1-py3-none-any.whl (34 kB)
#9 17.51 Downloading smmap-5.0.2-py3-none-any.whl (24 kB)
#9 17.53 Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)
#9 17.53    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.2/83.2 kB 27.7 MB/s eta 0:00:00
#9 17.55 Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)
#9 17.56    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.1/83.1 kB 26.3 MB/s eta 0:00:00
#9 17.57 Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)
#9 18.59 Installing collected packages: pytz, zipp, wrapt, urllib3, tzdata, typing-extensions, threadpoolctl, sqlparse, sniffio, smmap, six, PyYAML, pyparsing, pyasn1, pyarrow, protobuf, pluggy, pip, pillow, packaging, numpy, markupsafe, markdown, kiwisolver, joblib, itsdangerous, iniconfig, idna, h11, greenlet, graphql-core, fonttools, cycler, cloudpickle, click, charset-normalizer, certifi, cachetools, blinker, annotated-types, werkzeug, uvicorn, typing-inspection, SQLAlchemy, scipy, rsa, requests, python-dateutil, pytest, pydantic-core, pyasn1-modules, Mako, jinja2, importlib_metadata, gunicorn, graphql-relay, gitdb, deprecated, contourpy, anyio, starlette, scikit-learn, pydantic, pandas, opentelemetry-api, matplotlib, lightgbm, graphene, google-auth, gitpython, Flask, docker, alembic, opentelemetry-semantic-conventions, fastapi, databricks-sdk, opentelemetry-sdk, mlflow-skinny, mlflow
#9 20.43   Attempting uninstall: pip
#9 20.43     Found existing installation: pip 24.0
#9 20.48     Uninstalling pip-24.0:
#9 20.68       Successfully uninstalled pip-24.0
#9 41.33 Successfully installed Flask-3.1.1 Mako-1.3.10 PyYAML-6.0.2 SQLAlchemy-2.0.41 alembic-1.16.1 annotated-types-0.7.0 anyio-4.9.0 blinker-1.9.0 cachetools-5.5.2 certifi-2025.4.26 charset-normalizer-3.4.2 click-8.2.1 cloudpickle-3.1.1 contourpy-1.3.2 cycler-0.12.1 databricks-sdk-0.55.0 deprecated-1.2.18 docker-7.1.0 fastapi-0.115.12 fonttools-4.58.1 gitdb-4.0.12 gitpython-3.1.44 google-auth-2.40.2 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 greenlet-3.2.2 gunicorn-23.0.0 h11-0.16.0 idna-3.10 importlib_metadata-8.6.1 iniconfig-2.1.0 itsdangerous-2.2.0 jinja2-3.1.6 joblib-1.5.0 kiwisolver-1.4.8 lightgbm-4.6.0 markdown-3.8 markupsafe-3.0.2 matplotlib-3.10.3 mlflow-2.22.0 mlflow-skinny-2.22.0 numpy-2.2.6 opentelemetry-api-1.33.1 opentelemetry-sdk-1.33.1 opentelemetry-semantic-conventions-0.54b1 packaging-24.2 pandas-2.2.3 pillow-11.2.1 pip-25.1.1 pluggy-1.6.0 protobuf-6.31.0 pyarrow-19.0.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.5 pydantic-core-2.33.2 pyparsing-3.2.3 pytest-8.3.5 python-dateutil-2.9.0.post0 pytz-2025.2 requests-2.32.3 rsa-4.9.1 scikit-learn-1.6.1 scipy-1.15.3 six-1.17.0 smmap-5.0.2 sniffio-1.3.1 sqlparse-0.5.3 starlette-0.46.2 threadpoolctl-3.6.0 typing-extensions-4.13.2 typing-inspection-0.4.1 tzdata-2025.2 urllib3-2.4.0 uvicorn-0.34.2 werkzeug-3.1.3 wrapt-1.17.2 zipp-3.22.0
#9 41.33 WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
#9 DONE 43.0s

#10 [5/9] COPY API_Script.py .
#10 DONE 0.3s

#11 [6/9] COPY preprocessing_pipeline.py .
#11 DONE 0.2s

#12 [7/9] RUN echo ">>>> START OF COPIED preprocessing_pipeline.py CONTENT <<<<" &&     cat /app/preprocessing_pipeline.py &&     echo ">>>> END OF COPIED preprocessing_pipeline.py CONTENT <<<<"
#12 0.345 >>>> START OF COPIED preprocessing_pipeline.py CONTENT <<<<
#12 0.346 print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
#12 0.346 print("EXECUTING THIS VERSION OF preprocessing_pipeline.py - V_DEBUG_001")
#12 0.346 print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
#12 0.346 
#12 0.346 # --- Importation des librairies ---
#12 0.346 
#12 0.346 import pandas as pd
#12 0.346 import numpy as np
#12 0.346 
#12 0.346 import mlflow
#12 0.346 import re
#12 0.346 import os # Ensure this is present
#12 0.346 import traceback # MODIFICATION 1: ADD THIS IMPORT
#12 0.346 
#12 0.346 from functools import reduce
#12 0.346 
#12 0.346 from sklearn.preprocessing import LabelEncoder
#12 0.346 # from memory_profiler import profile # Ensure this is commented/removed
#12 0.346 
#12 0.346 # --- Définition des fonctions nécessaires à la préparation de l'input pour le modèle ---
#12 0.346 
#12 0.346 # ... (agg_numeric function - NO CHANGES) ...
#12 0.346 def agg_numeric(df, group_var, df_name):
#12 0.346     for col in df:
#12 0.346         if col != group_var and 'SK_ID' in col:
#12 0.346             df = df.drop(columns = col)
#12 0.346     group_ids = df[group_var]
#12 0.346     numeric_df = df.select_dtypes('number')
#12 0.346     numeric_df[group_var] = group_ids
#12 0.346     agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()
#12 0.346     columns = [group_var]
#12 0.346     for var in agg.columns.levels[0]:
#12 0.346         if var != group_var:
#12 0.346             for stat in agg.columns.levels[1][:-1]:
#12 0.346                 columns.append('%s_%s_%s' % (df_name, var, stat))
#12 0.346     agg.columns = columns
#12 0.346     return agg
#12 0.346 
#12 0.346 # ... (count_categorical function - NO CHANGES) ...
#12 0.346 def count_categorical(df, group_var, df_name):
#12 0.346     categorical = pd.get_dummies(df.select_dtypes('object'))
#12 0.346     categorical[group_var] = df[group_var]
#12 0.346     categorical = categorical.groupby(group_var).agg(['sum', 'mean'])
#12 0.346     column_names = []
#12 0.346     for var in categorical.columns.levels[0]:
#12 0.346         for stat in ['count', 'count_norm']:
#12 0.346             column_names.append('%s_%s_%s' % (df_name, var, stat))
#12 0.346     categorical.columns = column_names
#12 0.346     return categorical
#12 0.346 
#12 0.346 # ... (sanitize_lgbm_colname function - NO CHANGES) ...
#12 0.346 def sanitize_lgbm_colname(colname):
#12 0.346     colname_str = str(colname)
#12 0.346     sanitized = re.sub(r'[\[\]{}":\',.<>\s/?!@#$%^&*()+=-]+', '_', colname_str)
#12 0.346     sanitized = re.sub(r'_+', '_', sanitized)
#12 0.346     sanitized = sanitized.strip('_')
#12 0.346     if not sanitized:
#12 0.346         sanitized = f"col_{hash(colname_str)}"
#12 0.346     if sanitized[0].isdigit():
#12 0.346         sanitized = '_' + sanitized
#12 0.346     return sanitized
#12 0.346 
#12 0.346 # --- Création de la fonction préparant les données d'entrée du modèle ---
#12 0.346 # @profile # Ensure commented out
#12 0.346 def prepare_input_data(current_app, bureau, bureau_balance, previous_application, POS_CASH_balance, installments_payments, credit_card_balance):
#12 0.346     """
#12 0.346 
#12 0.346     Receives application data and aggregates each DataFrame then combines them into the expected input for the scoring model
#12 0.346 
#12 0.346     Parameters
#12 0.346     --------
#12 0.346 
#12 0.346     current_app, bureau, bureau_balance, previous_application, POS_CASH_balance, installment_payments, credit_card_balance :
#12 0.346     The 7 DataFrames used for the feature engineering process
#12 0.346 
#12 0.346     Return
#12 0.346     --------
#12 0.346     client_data : dataframe
#12 0.346         A dataframe with the required features matching the model's expected input features
#12 0.346     
#12 0.346     """
#12 0.346 
#12 0.346     # --- Preparation de current_app ---
#12 0.346 
#12 0.346     # Label encoding des variables catégorielles avec 2 catégories uniques ou moins
#12 0.346     le = LabelEncoder()
#12 0.346 
#12 0.346     for col in current_app:
#12 0.346         if current_app[col].dtype == 'object':
#12 0.346             # If 2 or fewer unique categories
#12 0.346             if len(list(current_app[col].unique())) <= 2:
#12 0.346                 # Train on the training data
#12 0.346                 le.fit(current_app[col])
#12 0.346                 # Transform both training and testing data
#12 0.346                 current_app[col] = le.transform(current_app[col])
#12 0.346     
#12 0.346     # One hot encoding pour le reste des variables catégorielles
#12 0.346     current_app = pd.get_dummies(current_app, dtype='float64')
#12 0.346 
#12 0.346     # --- Aggrégation de bureau et bureau_balance ---
#12 0.346     
#12 0.346     # Création des features "manuelles"
#12 0.346     bureau_balance_loan_duration_months = bureau_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].count().reset_index()
#12 0.346     bureau_balance_loan_duration_months.rename(columns = {'MONTHS_BALANCE' : 'MONTHS_LOAN_DURATION'}, inplace=True)
#12 0.346 
#12 0.346     bureau_balance_last_known_loan_status = bureau_balance.sort_values(by='MONTHS_BALANCE', ascending=False).groupby('SK_ID_BUREAU').first().reset_index()[['SK_ID_BUREAU', 'STATUS']]
#12 0.346     #bureau_balance_last_known_loan_status = pd.get_dummies(bureau_balance_last_known_loan_status, dtype=int)
#12 0.346     all_expected_status_categories = ['0', '1', '2', '3', '4', '5', 'C', 'X'] # These are the raw STATUS values
#12 0.346     for cat_col_name in all_expected_status_categories:
#12 0.346         if cat_col_name not in bureau_balance_last_known_loan_status.columns:
#12 0.346             bureau_balance_last_known_loan_status[cat_col_name] = 0
#12 0.346 
#12 0.346     bureau_balance_for_dpd_flag = bureau_balance.copy()
#12 0.346     non_dpd_statuses = ['C', 'X', '0']
#12 0.346     dpd_flagged = ~bureau_balance_for_dpd_flag['STATUS'].isin(non_dpd_statuses) #Cree True/False value en fonction de la condition ici dpd --> True
#12 0.346     bureau_balance_for_dpd_flag['DPD_FLAG'] = dpd_flagged.astype(int)
#12 0.346     bureau_balance_nombre_delais_de_paiements = bureau_balance_for_dpd_flag.groupby(by='SK_ID_BUREAU')['DPD_FLAG'].sum().reset_index()
#12 0.346 
#12 0.346     bureau_balance_nombre_de_delais_de_paiements_par_categorie = bureau_balance.groupby('SK_ID_BUREAU')['STATUS'].value_counts().unstack(fill_value=0).reset_index()
#12 0.346     bureau_balance_nombre_de_delais_de_paiements_par_categorie.columns.name = None
#12 0.346 
#12 0.346     Values_to_map = {'C':0, 'X':0, '0':0, '1':15, '2':45, '3':75, '4':105, '5':120}
#12 0.346     bureau_balance_for_mean_delay_calc = bureau_balance.copy()
#12 0.346     bureau_balance_for_mean_delay_calc['MEAN_DAYS_PAST_DUE'] = bureau_balance_for_mean_delay_calc['STATUS'].map(Values_to_map).fillna('Unknown')
#12 0.346     bureau_balance_duree_moyenne_delais_de_paiements = bureau_balance_for_mean_delay_calc.groupby('SK_ID_BUREAU')['MEAN_DAYS_PAST_DUE'].mean().reset_index()
#12 0.346 
#12 0.346     bureau_balance_loan_duration_categorised = bureau_balance.sort_values(by='MONTHS_BALANCE', ascending=True).groupby('SK_ID_BUREAU').first().reset_index()
#12 0.346     bureau_balance_loan_duration_categorised['YEAR_LOAN_DURATION'] = round(bureau_balance_loan_duration_categorised['MONTHS_BALANCE']/(-12), 1)
#12 0.346     bureau_balance_loan_duration_categorised['LOAN_TYPE'] = np.where(
#12 0.346         bureau_balance_loan_duration_categorised['YEAR_LOAN_DURATION'] >= 5, # Condition
#12 0.346         'Long Term',                                             # Value if True
#12 0.346         'Short Term'                                             # Value if False
#12 0.346     )
#12 0.346     bureau_balance_loan_duration_categorised = bureau_balance_loan_duration_categorised.drop(columns=['MONTHS_BALANCE', 'STATUS'], axis=0)
#12 0.346     bureau_balance_loan_duration_categorised = pd.get_dummies(bureau_balance_loan_duration_categorised, dtype=int)
#12 0.346 
#12 0.346     dfs_to_merge = [bureau_balance_loan_duration_months,
#12 0.346     bureau_balance_last_known_loan_status,
#12 0.346     bureau_balance_nombre_delais_de_paiements,
#12 0.346     bureau_balance_nombre_de_delais_de_paiements_par_categorie,
#12 0.346     bureau_balance_duree_moyenne_delais_de_paiements,
#12 0.346     bureau_balance_loan_duration_categorised]
#12 0.346 
#12 0.346     merge_key = 'SK_ID_BUREAU'
#12 0.346 
#12 0.346     merge_function = lambda left_df, right_df: pd.merge(left_df, right_df, on=merge_key, how='inner')
#12 0.346     final_bureau_balance_features = reduce(merge_function, dfs_to_merge)
#12 0.346     #final_bureau_balance_features = final_bureau_balance_features.drop(columns=['0', 'C', 'X'])
#12 0.346 
#12 0.346 
#12 0.346     bureau_for_credit_status = bureau.copy()
#12 0.346     bureau_number_of_type_of_credits = bureau.groupby('SK_ID_CURR')['CREDIT_ACTIVE'].value_counts().unstack(fill_value=0).reset_index()
#12 0.346     bureau_number_of_type_of_credits.columns.name = None
#12 0.346 
#12 0.346     bureau_for_encoding_credit_types = bureau[['SK_ID_CURR', 'CREDIT_TYPE']]
#12 0.346     bureau_credit_type_encoded = pd.get_dummies(bureau_for_encoding_credit_types, dtype=int)
#12 0.346     bureau_liste_colonnes_encodees = bureau_credit_type_encoded.columns
#12 0.346     bureau_liste_colonnes_a_agreger = [col for col in bureau_liste_colonnes_encodees if col != 'SK_ID_CURR']
#12 0.346     bureau_credit_type_encoded_and_aggregated = bureau_credit_type_encoded.groupby('SK_ID_CURR')[bureau_liste_colonnes_a_agreger].sum().reset_index()
#12 0.346 
#12 0.346     bureau_for_max_amount_overdue = bureau[['SK_ID_CURR', 'AMT_CREDIT_MAX_OVERDUE']]
#12 0.346     bureau_for_max_amount_overdue.fillna(value=0, inplace=True)
#12 0.346     bureau_for_max_amount_overdue = bureau_for_max_amount_overdue.groupby('SK_ID_CURR')['AMT_CREDIT_MAX_OVERDUE'].max().reset_index()
#12 0.346 
#12 0.346     bureau_for_mean_amount_overdue_across_loans = bureau[['SK_ID_CURR', 'AMT_CREDIT_SUM_OVERDUE']]
#12 0.346     bureau_for_mean_amount_overdue_across_loans = bureau_for_mean_amount_overdue_across_loans.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_OVERDUE'].mean().reset_index()
#12 0.346 
#12 0.346     bureau_for_credit_prolonged_count = bureau[['SK_ID_CURR', 'CNT_CREDIT_PROLONG']]
#12 0.346     bureau_for_credit_prolonged_count = bureau_for_credit_prolonged_count.groupby('SK_ID_CURR')['CNT_CREDIT_PROLONG'].sum().reset_index()
#12 0.346 
#12 0.346     bureau_for_proportions_repaid = bureau[['SK_ID_CURR', 'SK_ID_BUREAU', 'AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM']]
#12 0.346     bureau_for_proportions_repaid.dropna(subset='AMT_CREDIT_SUM', inplace=True)
#12 0.346     bureau_for_proportions_repaid['AMT_CREDIT_SUM_DEBT_IS_MISSING'] = bureau_for_proportions_repaid['AMT_CREDIT_SUM_DEBT'].isnull().astype(int)
#12 0.346     mask_active_missing_debt = bureau_for_proportions_repaid['AMT_CREDIT_SUM_DEBT'].isnull()
#12 0.346     bureau_for_proportions_repaid.loc[mask_active_missing_debt, 'AMT_CREDIT_SUM_DEBT'] = bureau_for_proportions_repaid.loc[mask_active_missing_debt, 'AMT_CREDIT_SUM']
#12 0.346     bureau_for_proportions_repaid_with_nan_values_treated = bureau_for_proportions_repaid.copy()
#12 0.346     bureau_for_proportions_repaid_with_nan_values_treated = bureau_for_proportions_repaid_with_nan_values_treated.groupby('SK_ID_CURR')[['AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM']].sum().reset_index()
#12 0.346     bureau_for_proportions_repaid_with_nan_values_treated['PROPORTION_REPAID'] = np.nan
#12 0.346     # Rule 1: SUM = 0 and DEBT = 0  => Repaid = 0
#12 0.346     mask_zero_both = (bureau_for_proportions_repaid_with_nan_values_treated['AMT_CREDIT_SUM'] == 0) & (bureau_for_proportions_repaid_with_nan_values_treated['AMT_CREDIT_SUM_DEBT'] == 0)
#12 0.346     bureau_for_proportions_repaid_with_nan_values_treated.loc[mask_zero_both, 'PROPORTION_REPAID'] = 1.0
#12 0.346     # Rule 2: DEBT < 0 => Repaid = 1
#12 0.346     mask_neg_debt = bureau_for_proportions_repaid_with_nan_values_treated['AMT_CREDIT_SUM_DEBT'] < 0
#12 0.346     bureau_for_proportions_repaid_with_nan_values_treated.loc[mask_neg_debt, 'PROPORTION_REPAID'] = 1.0
#12 0.346     # Rule 4: DEBT>0 and SUM = 0
#12 0.346     mask_pos_debt_null_sum = (bureau_for_proportions_repaid_with_nan_values_treated['AMT_CREDIT_SUM'] == 0) & (bureau_for_proportions_repaid_with_nan_values_treated['AMT_CREDIT_SUM_DEBT'] > 0)
#12 0.346     bureau_for_proportions_repaid_with_nan_values_treated.loc[mask_pos_debt_null_sum, 'PROPORTION_REPAID'] = 0
#12 0.346     # Rule 4: All other cases where SUM > 0
#12 0.346     mask_calculate = (bureau_for_proportions_repaid_with_nan_values_treated['AMT_CREDIT_SUM'] > 0) & (bureau_for_proportions_repaid_with_nan_values_treated['PROPORTION_REPAID'].isnull()) # Only calculate where not set yet
#12 0.346     bureau_for_proportions_repaid_with_nan_values_treated.loc[mask_calculate, 'PROPORTION_REPAID'] = 1.0 - (bureau_for_proportions_repaid_with_nan_values_treated.loc[mask_calculate, 'AMT_CREDIT_SUM_DEBT'] / bureau_for_proportions_repaid_with_nan_values_treated.loc[mask_calculate, 'AMT_CREDIT_SUM'])
#12 0.346 
#12 0.346     bureau_for_deadline_in_the_past = bureau[['SK_ID_CURR', 'CREDIT_ACTIVE', 'DAYS_CREDIT_ENDDATE']].copy()
#12 0.346     bureau_for_deadline_in_the_past['LOAN_ACTIVE_PAST_DEADLINE'] = np.where(
#12 0.346         (bureau_for_deadline_in_the_past['CREDIT_ACTIVE'] == 'Active') & (bureau_for_deadline_in_the_past['DAYS_CREDIT_ENDDATE'] < 0), # Condition
#12 0.346         1,                                             # Value if True
#12 0.346         0                                             # Value if False
#12 0.346     )
#12 0.346     bureau_for_deadline_in_the_past = bureau_for_deadline_in_the_past.groupby('SK_ID_CURR')['LOAN_ACTIVE_PAST_DEADLINE'].sum().reset_index()
#12 0.346 
#12 0.346     bureau_for_mean_days_spent_overdue = bureau[['SK_ID_CURR', 'CREDIT_DAY_OVERDUE']]
#12 0.346     bureau_for_mean_days_spent_overdue = bureau_for_mean_days_spent_overdue.groupby('SK_ID_CURR')['CREDIT_DAY_OVERDUE'].mean().reset_index()
#12 0.346 
#12 0.346     dfs_to_merge = [bureau_number_of_type_of_credits,
#12 0.346     bureau_credit_type_encoded_and_aggregated,
#12 0.346     bureau_for_max_amount_overdue,
#12 0.346     bureau_for_mean_amount_overdue_across_loans,
#12 0.346     bureau_for_credit_prolonged_count,
#12 0.346     bureau_for_proportions_repaid_with_nan_values_treated,
#12 0.346     bureau_for_deadline_in_the_past,
#12 0.346     bureau_for_mean_days_spent_overdue]
#12 0.346 
#12 0.346     merge_key = 'SK_ID_CURR'
#12 0.346 
#12 0.346     merge_function = lambda left_df, right_df: pd.merge(left_df, right_df, on=merge_key, how='inner')
#12 0.346     final_bureau_features = reduce(merge_function, dfs_to_merge)
#12 0.346 
#12 0.346     ids_for_agg = bureau[['SK_ID_CURR', 'SK_ID_BUREAU']]
#12 0.346     final_bureau_balance_features_with_curr = pd.merge(left=ids_for_agg, right=final_bureau_balance_features, how='inner')
#12 0.346     all_expected_status_categories = ['0', '1', '2', '3', '4', '5', 'C', 'X'] # These are the raw STATUS values
#12 0.346     for cat_col_name in all_expected_status_categories:
#12 0.346         if cat_col_name not in final_bureau_balance_features_with_curr.columns:
#12 0.346             final_bureau_balance_features_with_curr[cat_col_name] = 0
#12 0.346     final_bureau_balance_features_with_curr_mean = final_bureau_balance_features_with_curr.groupby('SK_ID_CURR')[['MONTHS_LOAN_DURATION', 'DPD_FLAG', '1', '2', '3', '4', '5', 'MEAN_DAYS_PAST_DUE', 'YEAR_LOAN_DURATION']].mean().reset_index()
#12 0.346     rename_mean_dict = {col: 'MEAN_' + str(col) for col in final_bureau_balance_features_with_curr_mean.columns if col != 'SK_ID_CURR'}
#12 0.346     final_bureau_balance_features_with_curr_mean.rename(columns=rename_mean_dict, inplace=True)
#12 0.346 
#12 0.346 
#12 0.346 
#12 0.346     all_expected_status_categories = ['STATUS_0','STATUS_1', 'STATUS_2', 'STATUS_3', 'STATUS_4', 'STATUS_5', 'STATUS_C','STATUS_X'] # These are the raw STATUS values
#12 0.346     for cat_col_name in all_expected_status_categories:
#12 0.346         if cat_col_name not in final_bureau_balance_features_with_curr.columns:
#12 0.346             final_bureau_balance_features_with_curr[cat_col_name] = 0
#12 0.346 
#12 0.346     final_bureau_balance_features_with_curr_sum = final_bureau_balance_features_with_curr.groupby('SK_ID_CURR')[['STATUS_0','STATUS_1', 'STATUS_2', 'STATUS_3', 'STATUS_4', 'STATUS_5', 'STATUS_C','STATUS_X', 'DPD_FLAG', '1', '2', '3', '4', '5', 'LOAN_TYPE_Long Term', 'LOAN_TYPE_Short Term']].sum().reset_index()
#12 0.346     rename_sum_dict = {col: 'SUM_' + str(col) for col in final_bureau_balance_features_with_curr_sum.columns if col != 'SK_ID_CURR'}
#12 0.346     final_bureau_balance_features_with_curr_sum.rename(columns=rename_sum_dict, inplace=True)
#12 0.346 
#12 0.346     final_bureau_balance_features_with_curr_agg = pd.merge(left=final_bureau_balance_features_with_curr_mean, right=final_bureau_balance_features_with_curr_sum, how='inner', on='SK_ID_CURR')
#12 0.346     features_bureau_bureau_balance = pd.merge(left=final_bureau_features, right=final_bureau_balance_features_with_curr_agg, how='inner', on='SK_ID_CURR')
#12 0.346 
#12 0.346     application_train_with_bureau_and_bureau_balance = pd.merge(left=current_app, right=features_bureau_bureau_balance, how='left', on='SK_ID_CURR')
#12 0.346     """application_train_with_bureau_and_bureau_balance[['Active', 'Bad debt', 'Closed', 'Sold',
#12 0.346        'CREDIT_TYPE_Another type of loan', 'CREDIT_TYPE_Car loan',
#12 0.346        'CREDIT_TYPE_Cash loan (non-earmarked)', 'CREDIT_TYPE_Consumer credit',
#12 0.346        'CREDIT_TYPE_Credit card', 'CREDIT_TYPE_Interbank credit',
#12 0.346        'CREDIT_TYPE_Loan for business development',
#12 0.346        'CREDIT_TYPE_Loan for purchase of shares (margin lending)',
#12 0.346        'CREDIT_TYPE_Loan for the purchase of equipment',
#12 0.346        'CREDIT_TYPE_Loan for working capital replenishment',
#12 0.346        'CREDIT_TYPE_Microloan', 'CREDIT_TYPE_Mobile operator loan',
#12 0.346        'CREDIT_TYPE_Mortgage', 'CREDIT_TYPE_Real estate loan',
#12 0.346        'CREDIT_TYPE_Unknown type of loan', 'AMT_CREDIT_MAX_OVERDUE',
#12 0.346        'AMT_CREDIT_SUM_OVERDUE', 'CNT_CREDIT_PROLONG', 'AMT_CREDIT_SUM_DEBT',
#12 0.346        'AMT_CREDIT_SUM', 'PROPORTION_REPAID', 'LOAN_ACTIVE_PAST_DEADLINE',
#12 0.346        'CREDIT_DAY_OVERDUE', 'MEAN_MONTHS_LOAN_DURATION', 'MEAN_DPD_FLAG',
#12 0.346        'MEAN_1', 'MEAN_2', 'MEAN_3', 'MEAN_4', 'MEAN_5',
#12 0.346        'MEAN_MEAN_DAYS_PAST_DUE', 'MEAN_YEAR_LOAN_DURATION', 'SUM_STATUS_0',
#12 0.346        'SUM_STATUS_1', 'SUM_STATUS_2', 'SUM_STATUS_3', 'SUM_STATUS_4',
#12 0.346        'SUM_STATUS_5', 'SUM_STATUS_C', 'SUM_STATUS_X', 'SUM_DPD_FLAG', 'SUM_1',
#12 0.346        'SUM_2', 'SUM_3', 'SUM_4', 'SUM_5', 'SUM_LOAN_TYPE_Long Term',
#12 0.346        'SUM_LOAN_TYPE_Short Term']].fillna(value=0, inplace=True)
#12 0.346     cols_to_fill = ['Active', 'Bad debt', 'Closed', 'Sold',
#12 0.346        'CREDIT_TYPE_Another type of loan', 'CREDIT_TYPE_Car loan',
#12 0.346        'CREDIT_TYPE_Cash loan (non-earmarked)', 'CREDIT_TYPE_Consumer credit',
#12 0.346        'CREDIT_TYPE_Credit card', 'CREDIT_TYPE_Interbank credit',
#12 0.346        'CREDIT_TYPE_Loan for business development',
#12 0.346        'CREDIT_TYPE_Loan for purchase of shares (margin lending)',
#12 0.346        'CREDIT_TYPE_Loan for the purchase of equipment',
#12 0.346        'CREDIT_TYPE_Loan for working capital replenishment',
#12 0.346        'CREDIT_TYPE_Microloan', 'CREDIT_TYPE_Mobile operator loan',
#12 0.346        'CREDIT_TYPE_Mortgage', 'CREDIT_TYPE_Real estate loan',
#12 0.346        'CREDIT_TYPE_Unknown type of loan', 'AMT_CREDIT_MAX_OVERDUE',
#12 0.346        'AMT_CREDIT_SUM_OVERDUE', 'CNT_CREDIT_PROLONG', 'AMT_CREDIT_SUM_DEBT',
#12 0.346        'AMT_CREDIT_SUM', 'PROPORTION_REPAID', 'LOAN_ACTIVE_PAST_DEADLINE',
#12 0.346        'CREDIT_DAY_OVERDUE', 'MEAN_MONTHS_LOAN_DURATION', 'MEAN_DPD_FLAG',
#12 0.346        'MEAN_1', 'MEAN_2', 'MEAN_3', 'MEAN_4', 'MEAN_5',
#12 0.346        'MEAN_MEAN_DAYS_PAST_DUE', 'MEAN_YEAR_LOAN_DURATION', 'SUM_STATUS_0',
#12 0.346        'SUM_STATUS_1', 'SUM_STATUS_2', 'SUM_STATUS_3', 'SUM_STATUS_4',
#12 0.346        'SUM_STATUS_5', 'SUM_STATUS_C', 'SUM_STATUS_X', 'SUM_DPD_FLAG', 'SUM_1',
#12 0.346        'SUM_2', 'SUM_3', 'SUM_4', 'SUM_5', 'SUM_LOAN_TYPE_Long Term',
#12 0.346        'SUM_LOAN_TYPE_Short Term']
#12 0.346     application_train_with_bureau_and_bureau_balance[cols_to_fill] = application_train_with_bureau_and_bureau_balance[cols_to_fill].fillna(value=0)"""
#12 0.346 
#12 0.346     
#12 0.346     # Création des features à l'aide des deux fonctions d'aggrégation
#12 0.346     num_features_with_function_bureau = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'bureau')
#12 0.346     cat_features_with_function_bureau = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')
#12 0.346 
#12 0.346     num_features_with_function_bureau_balance = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')
#12 0.346     cat_features_with_function_bureau_balance = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')
#12 0.346 
#12 0.346     num_and_cat_features_with_function_bureau_balance = pd.merge(num_features_with_function_bureau_balance, cat_features_with_function_bureau_balance, how='inner', on='SK_ID_BUREAU')
#12 0.346     ids_for_agg = bureau[['SK_ID_CURR', 'SK_ID_BUREAU']]
#12 0.346     num_and_cat_features_with_function_bureau_balance_with_curr = pd.merge(left=ids_for_agg, right=num_and_cat_features_with_function_bureau_balance, how='inner').sort_values(by='SK_ID_BUREAU', ascending=True)
#12 0.346     num_and_cat_features_with_function_bureau_balance_with_curr_agg = agg_numeric(num_and_cat_features_with_function_bureau_balance_with_curr.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')
#12 0.346     manual_final_bureau_balance_features_with_curr_agg = agg_numeric(final_bureau_balance_features_with_curr.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client_bureau_balance')
#12 0.346 
#12 0.346     dfs_to_merge = [num_features_with_function_bureau,
#12 0.346     cat_features_with_function_bureau,
#12 0.346     final_bureau_features,
#12 0.346     num_and_cat_features_with_function_bureau_balance_with_curr_agg,
#12 0.346     manual_final_bureau_balance_features_with_curr_agg]
#12 0.346 
#12 0.346     merge_key = 'SK_ID_CURR'
#12 0.346 
#12 0.346     merge_function = lambda left_df, right_df: pd.merge(left_df, right_df, on=merge_key, how='left')
#12 0.346     features_manual_and_func_from_first_three_without_app_train = reduce(merge_function, dfs_to_merge)
#12 0.346 
#12 0.346     features_manual_and_func_from_first_three_with_app_train = pd.merge(left=current_app, right=features_manual_and_func_from_first_three_without_app_train, how='left', on='SK_ID_CURR')
#12 0.346 
#12 0.346     # Aggrégation à l'aide des fonctions de previous_application, POS_CASH_balance, installments_payments et credit_card_balance
#12 0.346     previous_application_num_agg_SK_ID_CURR = agg_numeric(previous_application.drop(columns=['SK_ID_PREV']), group_var='SK_ID_CURR', df_name='previous_application')
#12 0.346     previous_application_cat_agg_SK_ID_CURR = count_categorical(previous_application.drop(columns=['SK_ID_PREV']), group_var='SK_ID_CURR', df_name='previous_application')
#12 0.346 
#12 0.346     POS_CASH_balance_num_agg_SK_ID_CURR = agg_numeric(POS_CASH_balance.drop(columns=['SK_ID_PREV']), group_var='SK_ID_CURR', df_name='POS_CASH_balance')
#12 0.346     POS_CASH_balance_cat_agg_SK_ID_CURR = count_categorical(POS_CASH_balance.drop(columns=['SK_ID_PREV']), group_var='SK_ID_CURR', df_name='POS_CASH_balance')
#12 0.346 
#12 0.346     credit_card_balance_num_agg_SK_ID_CURR = agg_numeric(credit_card_balance.drop(columns=['SK_ID_PREV']), group_var='SK_ID_CURR', df_name='credit_card_balance')
#12 0.346     credit_card_balance_cat_agg_SK_ID_CURR = count_categorical(credit_card_balance.drop(columns=['SK_ID_PREV']), group_var='SK_ID_CURR', df_name='credit_card_balance')
#12 0.346 
#12 0.346     installments_payments_num_agg_SK_ID_CURR = agg_numeric(installments_payments.drop(columns=['SK_ID_PREV']), group_var='SK_ID_CURR', df_name='credit_card_balance')
#12 0.346 
#12 0.346     dfs_to_merge = [installments_payments_num_agg_SK_ID_CURR, 
#12 0.346                 previous_application_num_agg_SK_ID_CURR, previous_application_cat_agg_SK_ID_CURR, 
#12 0.346                 POS_CASH_balance_num_agg_SK_ID_CURR, POS_CASH_balance_cat_agg_SK_ID_CURR, 
#12 0.346                 credit_card_balance_num_agg_SK_ID_CURR, credit_card_balance_cat_agg_SK_ID_CURR]
#12 0.346     
#12 0.346     merge_key = 'SK_ID_CURR'
#12 0.346 
#12 0.346     merge_function = lambda left_df, right_df: pd.merge(left_df, right_df, on=merge_key, how='left')
#12 0.346     features_func_from_last_four = reduce(merge_function, dfs_to_merge)
#12 0.346     full_data = pd.merge(left=features_manual_and_func_from_first_three_with_app_train, right=features_func_from_last_four, how='left', on='SK_ID_CURR')
#12 0.346     print("\nDataset was properly aggregated.")
#12 0.346 
#12 0.346     return full_data
#12 0.346 
#12 0.346 # @profile # Ensure commented out
#12 0.346 def predicting_scores(full_data):
#12 0.346     """
#12 0.346     Uses the preprocessed data to predict wether or not a client is likely to repay the loan they applied for.
#12 0.346 
#12 0.346     Parameters
#12 0.346     --------
#12 0.346         full_data (dataframe):
#12 0.346             The DataFrame containing all of the features the scoring model needs
#12 0.346 
#12 0.346     Return
#12 0.346     --------
#12 0.346         client_with_scores (dataframe) containing all of the SK_ID_CURR and the score predicted for each
#12 0.346         likely_to_repay(dataframe) containing all of the SK_ID_CURR with a score strictly below the optimal threshold,
#12 0.346         not_likely_to_repay(dataframe) containing all of the SK_ID_CURR with a score equal or above the optimal threshold
#12 0.346 
#12 0.346     """
#12 0.346     print("--- Inside predicting_scores (Per-Request Model Load Version with Path Debugging) ---")
#12 0.346 
#12 0.346     run_id_of_the_pipeline = "dc6fb5c5f64e4a90b08ca76d9b962765"
#12 0.346     pipeline_artifact_path = "full_lgbm_pipeline" # This name is confirmed correct
#12 0.346 
#12 0.346     # --- Start of MLflow Path Debugging ---
#12 0.346     current_tracking_uri = mlflow.get_tracking_uri()
#12 0.346     expected_tracking_uri_in_container = "file:./mlruns"
#12 0.346     # MLflow might resolve ./mlruns to an absolute path from WORKDIR, so check both
#12 0.346     # WORKDIR is /app in the Dockerfile
#12 0.346     if current_tracking_uri != expected_tracking_uri_in_container and current_tracking_uri != "file:/app/mlruns":
#12 0.346         print(f"[DEBUG] Current MLflow tracking URI is '{current_tracking_uri}'. Forcing to '{expected_tracking_uri_in_container}'.")
#12 0.346         try:
#12 0.346             mlflow.set_tracking_uri(expected_tracking_uri_in_container)
#12 0.346             current_tracking_uri = mlflow.get_tracking_uri() # Update after setting
#12 0.346         except Exception as e_set_uri:
#12 0.346             print(f"[DEBUG] ERROR setting tracking URI: {e_set_uri}")
#12 0.346     print(f"[DEBUG] Using MLflow tracking URI: {current_tracking_uri}")
#12 0.346 
#12 0.346     pipeline_model_uri = f"runs:/{run_id_of_the_pipeline}/{pipeline_artifact_path}"
#12 0.346     print(f"[DEBUG] Constructed MLflow Model URI: {pipeline_model_uri}")
#12 0.346 
#12 0.346     experiment_id_for_path = "448441841985485771" # From your run's directory structure
#12 0.346 
#12 0.346     print("[DEBUG] --- Verifying Expected File System Structure Inside Container ---")
#12 0.346     base_dir_to_check = "/app" # This is the WORKDIR
#12 0.346     print(f"[DEBUG] Checking base WORKDIR: {base_dir_to_check}. Exists: {os.path.exists(base_dir_to_check)}")
#12 0.346     if os.path.exists(base_dir_to_check):
#12 0.346         try:
#12 0.346             print(f"[DEBUG] Contents of {base_dir_to_check} (first 10): {os.listdir(base_dir_to_check)[:10]}")
#12 0.346         except Exception as e_ls_base:
#12 0.346             print(f"[DEBUG] Error listing contents of {base_dir_to_check}: {e_ls_base}")
#12 0.346 
#12 0.346 
#12 0.346     paths_to_check = {
#12 0.346         "mlruns_dir_in_workdir": os.path.join(base_dir_to_check, "mlruns"), # Should be /app/mlruns
#12 0.346         "experiment_dir": os.path.join(base_dir_to_check, "mlruns", experiment_id_for_path),
#12 0.346         "run_dir": os.path.join(base_dir_to_check, "mlruns", experiment_id_for_path, run_id_of_the_pipeline),
#12 0.346         "run_artifacts_dir": os.path.join(base_dir_to_check, "mlruns", experiment_id_for_path, run_id_of_the_pipeline, "artifacts"),
#12 0.346         "model_artifact_dir": os.path.join(base_dir_to_check, "mlruns", experiment_id_for_path, run_id_of_the_pipeline, "artifacts", pipeline_artifact_path)
#12 0.346     }
#12 0.346 
#12 0.346     for name, path_to_check in paths_to_check.items():
#12 0.346         # For file system checks, it's usually better to work with absolute paths if you know the base.
#12 0.346         # os.path.abspath() might behave unexpectedly if the path doesn't start with / inside container.
#12 0.346         # Since we know base_dir_to_check is /app, the joined paths are already absolute.
#12 0.346         abs_path_to_check = path_to_check # The joined paths are already absolute starting from /app
#12 0.346         print(f"[DEBUG] Checking: {name} at resolved path: {abs_path_to_check}")
#12 0.346         if os.path.exists(abs_path_to_check):
#12 0.346             print(f"[DEBUG] ==> Path EXISTS: {abs_path_to_check}")
#12 0.346             try:
#12 0.346                 if os.path.isdir(abs_path_to_check):
#12 0.346                     contents = os.listdir(abs_path_to_check)
#12 0.346                     print(f"[DEBUG]     Contents (first 5 if many): {contents[:5]}")
#12 0.346                     # Specifically for model_artifact_dir, check for MLmodel
#12 0.346                     if name == "model_artifact_dir":
#12 0.346                         mlmodel_file_path = os.path.join(abs_path_to_check, "MLmodel")
#12 0.346                         print(f"[DEBUG]     Checking for MLmodel file at: {mlmodel_file_path}. Exists: {os.path.exists(mlmodel_file_path)}")
#12 0.346                 else: # Should be a directory, but good to note if it's a file
#12 0.346                     print(f"[DEBUG]     Note: Path is a FILE, not a directory.")
#12 0.346             except Exception as e_ls:
#12 0.346                 print(f"[DEBUG]     Error listing contents of {abs_path_to_check}: {e_ls}")
#12 0.346         else:
#12 0.346             print(f"[DEBUG] ==> Path DOES NOT EXIST: {abs_path_to_check}")
#12 0.346     print("[DEBUG] --- End of File System Structure Verification ---")
#12 0.346     # --- End of MLflow Path Debugging ---
#12 0.346 
#12 0.346     # --- Get Model Info and Signature ---
#12 0.346     expected_input_columns = None # Initialize
#12 0.346     try:
#12 0.346         print(f"Fetching model info for URI: {pipeline_model_uri}") # This is your original print
#12 0.346         model_info = mlflow.models.get_model_info(pipeline_model_uri)
#12 0.346         if model_info.signature is not None and model_info.signature.inputs is not None:
#12 0.346             input_schema_obj = model_info.signature.inputs
#12 0.346             if hasattr(input_schema_obj, 'input_names') and callable(getattr(input_schema_obj, 'input_names')):
#12 0.346                 expected_input_columns = input_schema_obj.input_names()
#12 0.346                 print("\nExpected input columns for the pipeline (from signature via input_names()):")
#12 0.346             elif hasattr(input_schema_obj, 'inputs') and isinstance(input_schema_obj.inputs, list):
#12 0.346                 expected_input_columns = [col_spec.name for col_spec in input_schema_obj.inputs]
#12 0.346                 print("\nExpected input columns for the pipeline (from signature via iterating inputs):")
#12 0.346             else:
#12 0.346                 print("\nCould not extract column names from input schema. Schema details:")
#12 0.346                 print(str(input_schema_obj)[:1000])
#12 0.346                 # expected_input_columns = None # Already initialized
#12 0.346         else:
#12 0.346             print("\nNo input signature or inputs found in model_info.")
#12 0.346             # expected_input_columns = None # Already initialized
#12 0.346     except AttributeError as ae:
#12 0.346         print(f"AttributeError while processing MLflow signature: {ae}")
#12 0.346         # expected_input_columns = None # Already initialized
#12 0.346     except Exception as e: # Catching a broader exception for model_info fetching
#12 0.346         print(f"Error fetching model info or signature from MLflow: {e}")
#12 0.346         # expected_input_columns = None # Already initialized
#12 0.346 
#12 0.346     if expected_input_columns is None:
#12 0.346         print("WARNING: Could not load expected_input_columns from MLflow signature. Attempting to load from 'pipeline_input_columns.txt'")
#12 0.346         try:
#12 0.346             # Inside Docker, paths are relative to WORKDIR unless absolute
#12 0.346             # WORKDIR is /app, so 'pipeline_input_columns.txt' should be /app/pipeline_input_columns.txt
#12 0.346             column_file_path = 'pipeline_input_columns.txt'
#12 0.346             print(f"[DEBUG] Attempting to open column file at: {os.path.abspath(column_file_path)}")
#12 0.346             with open(column_file_path, 'r') as f:
#12 0.346                 loaded_cols = [line.strip() for line in f if line.strip()]
#12 0.346             if not loaded_cols:
#12 0.346                 raise ValueError("pipeline_input_columns.txt is empty or contains no valid column names.")
#12 0.346             expected_input_columns = loaded_cols
#12 0.346             print(f"Successfully loaded {len(expected_input_columns)} expected input columns from {column_file_path}")
#12 0.346         except FileNotFoundError:
#12 0.346             print(f"ERROR: {column_file_path} not found (resolved to {os.path.abspath(column_file_path)}). This file is required as a fallback if MLflow signature fails.")
#12 0.346             raise
#12 0.346         except ValueError as ve:
#12 0.346             print(f"ERROR loading from {column_file_path}: {ve}")
#12 0.346             raise
#12 0.346 
#12 0.346     if expected_input_columns is None:
#12 0.346         print("CRITICAL ERROR: expected_input_columns is still None after all loading attempts. Cannot proceed.")
#12 0.346         raise ValueError("Failed to obtain expected input columns for the model.")
#12 0.346     # expected_columns_set = set(expected_input_columns) # This can be defined before column alignment
#12 0.346 
#12 0.346     # --- Model Loading with Debug ---
#12 0.346     try:
#12 0.346         print(f"[DEBUG] About to call mlflow.sklearn.load_model('{pipeline_model_uri}')")
#12 0.346         loaded_pipeline = mlflow.sklearn.load_model(pipeline_model_uri)
#12 0.346         print(f"\n[DEBUG] Pipeline loaded successfully via mlflow.sklearn.load_model. Type: {type(loaded_pipeline)}")
#12 0.346     except Exception as e_load_model:
#12 0.346         print(f"[DEBUG] ERROR during mlflow.sklearn.load_model: {e_load_model}")
#12 0.346         traceback_str = traceback.format_exc()
#12 0.346         print(f"[DEBUG] Traceback for load_model error:\n{traceback_str}")
#12 0.346         raise # Re-raise the error
#12 0.346     # --- End of Model Loading with Debug ---
#12 0.346 
#12 0.346     # --- Original processing & prediction logic (ACTIVE) ---
#12 0.346     print("\n(Original Log) Pipeline loaded successfully.") # Your original log line
#12 0.346     previously_fitted_scaler = loaded_pipeline.named_steps['standardscaler']
#12 0.346     print("\n(Original Log) Scaler object extracted from the pipeline.")
#12 0.346 
#12 0.346     print("Sanitizing full_data column names...")
#12 0.346     original_train_cols = full_data.columns.tolist()
#12 0.346     # Need to define sanitize_lgbm_colname or ensure it's imported if it's elsewhere
#12 0.346     new_sanitized_cols = [sanitize_lgbm_colname(col) for col in original_train_cols]
#12 0.346     full_data.columns = new_sanitized_cols
#12 0.346     print("Column names sanitized")
#12 0.346 
#12 0.346     if full_data.empty or 'SK_ID_CURR' not in full_data.columns:
#12 0.346         print("ERROR: full_data is empty or SK_ID_CURR is missing before prediction alignment. Cannot proceed.")
#12 0.346         raise ValueError("full_data is empty or missing SK_ID_CURR after prepare_input_data.")
#12 0.346     SK_ID_CURR_current_app_batch = full_data["SK_ID_CURR"].copy()
#12 0.346 
#12 0.346     current_columns = set(full_data.columns)
#12 0.346     expected_columns_set = set(expected_input_columns)
#12 0.346     columns_to_drop = list(current_columns - expected_columns_set)
#12 0.346     columns_to_drop_existing = [col for col in columns_to_drop if col in full_data.columns]
#12 0.346     if columns_to_drop_existing:
#12 0.346         full_data.drop(columns=columns_to_drop_existing, inplace=True)
#12 0.346     print(f"Extra columns dropped: {columns_to_drop_existing if columns_to_drop_existing else 'None'}")
#12 0.346 
#12 0.346     columns_to_add = list(expected_columns_set - set(full_data.columns))
#12 0.346     for col in columns_to_add:
#12 0.346         full_data[col] = np.nan
#12 0.346     print(f"Missing columns added with NaN values: {columns_to_add if columns_to_add else 'None'}")
#12 0.346 
#12 0.346     print("\nReordering columns to match the expected order...")
#12 0.346     # Ensure all expected_input_columns are present in full_data before reordering
#12 0.346     missing_for_reorder = [col for col in expected_input_columns if col not in full_data.columns]
#12 0.346     if missing_for_reorder:
#12 0.346         print(f"ERROR: Columns expected for reordering are missing from full_data: {missing_for_reorder}")
#12 0.346         raise ValueError(f"Cannot reorder. Columns missing: {missing_for_reorder}")
#12 0.346     full_data = full_data[expected_input_columns]
#12 0.346     print("Columns reordered.")
#12 0.346 
#12 0.346     if list(full_data.columns) == expected_input_columns:
#12 0.346         print("\nVerification successful: Columns in full_data now match expected_input_columns in name and order.")
#12 0.346     else:
#12 0.346         print("\nWARNING: Column alignment might not be perfect. Please review.")
#12 0.346         print(f"  Actual columns (first 10):   {list(full_data.columns)[:10]}")
#12 0.346         print(f"  Expected columns (first 10): {expected_input_columns[:10]}")
#12 0.346 
#12 0.346     print("\nMaking predictions using the loaded pipeline...")
#12 0.346     # Ensure full_data is not empty before predict_proba
#12 0.346     if full_data.empty:
#12 0.346         print("ERROR: full_data is empty before calling predict_proba. This should not happen if input current_app was not empty.")
#12 0.346         # Create empty results based on expected output structure
#12 0.346         # This depends on how your API needs to respond to "no data to predict"
#12 0.346         # For now, raising an error is clearer for debugging
#12 0.346         raise ValueError("full_data became empty before prediction, check alignment and input.")
#12 0.346 
#12 0.346     current_app_scores = loaded_pipeline.predict_proba(full_data)[:, 1]
#12 0.346     print("Probabilities predicted successfully.")
#12 0.346 
#12 0.346     current_app_scores = current_app_scores*100
#12 0.346     print(f"Scores (first 5): {current_app_scores[:5]}")
#12 0.346 
#12 0.346     optimal_threshold = 0.6336 * 100
#12 0.346     scores_DataFrame = pd.DataFrame({'SCORE': current_app_scores}, index=SK_ID_CURR_current_app_batch.index)
#12 0.346     client_with_scores = pd.concat([SK_ID_CURR_current_app_batch, scores_DataFrame], axis=1)
#12 0.346     likely_to_repay = client_with_scores[client_with_scores['SCORE'] < optimal_threshold]
#12 0.346     print(f"Likely to repay (first 5 heads if any):\n{likely_to_repay.head().to_string()}")
#12 0.346     not_likely_to_repay = client_with_scores[client_with_scores['SCORE'] >= optimal_threshold]
#12 0.346     print(f"Not likely to repay (first 5 heads if any):\n{not_likely_to_repay.head().to_string()}")
#12 0.346 
#12 0.346     return client_with_scores, likely_to_repay, not_likely_to_repay>>>> END OF COPIED preprocessing_pipeline.py CONTENT <<<<
#12 DONE 0.4s

#13 [8/9] COPY mlruns ./mlruns
#13 DONE 0.5s

#14 [9/9] COPY pipeline_input_columns.txt .
#14 DONE 0.2s

#15 exporting to image
#15 exporting layers
#15 exporting layers 28.8s done
#15 exporting manifest sha256:ce0d59edcbfe15222289922ce0bc58601add7993e3047dcd1fb4672491a70d25 0.0s done
#15 exporting config sha256:749c113c398436663cfa3030fddd9f8a89e5b4bff923d5f40775f5d191b7f94d done
#15 exporting attestation manifest sha256:d1f5e2c57dd42b1a5999bb3d744e4bfd56c9a01549fc7a0bffe24cdce6406655 0.0s done
#15 exporting manifest list sha256:18692816ffc13eae24c24e92d1e1bdde573e5a7f226e93eb451388e1a600cbcb done
#15 naming to docker.io/library/credit-scoring-api:latest done
#15 unpacking to docker.io/library/credit-scoring-api:latest
#15 unpacking to docker.io/library/credit-scoring-api:latest 9.5s done
#15 DONE 38.4s
